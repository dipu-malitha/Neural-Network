{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO7Skl87Ifin1q9UbTlwyf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipu-malitha/Neural-Network/blob/main/Forward_and_Backpropagation_Example_for_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a network with:\n",
        "\n",
        "2 input neurons\n",
        "\n",
        "1 hidden layer with 2 neurons (sigmoid activation)\n",
        "\n",
        "1 output neuron (sigmoid activation)"
      ],
      "metadata": {
        "id": "ofO32O8nIDPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Parameters:\n",
        "Input: x = [0.5, 0.1]\n",
        "\n",
        "True label: y = 1\n",
        "\n",
        "Weights:\n",
        "\n",
        "W1 (input to hidden): [[0.1, 0.2], [0.3, 0.4]]\n",
        "\n",
        "W2 (hidden to output): [[0.5, 0.6]]\n",
        "\n",
        "Bias:\n",
        "\n",
        "b1 (hidden): [0.1, 0.2]\n",
        "\n",
        "b2 (output): [0.3]"
      ],
      "metadata": {
        "id": "EFmPwo-RIax8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhP6GKOxH_VX",
        "outputId": "bb65cc45-9be2-4d4e-b0c6-8a8ca0a8db60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward Pass Results: [0.71564357]\n",
            "Hidden Output: [0.54487889 0.58419052]\n",
            "\n",
            "Gradients:\n",
            "dL_dW1:\n",
            "[[-0.0176291  -0.02072207]\n",
            " [-0.00352582 -0.00414441]]\n",
            "dL_db1: [-0.03525819 -0.04144415]\n",
            "dL_dW2: [[-0.15493982 -0.16611833]]\n",
            "dL_db2: [-0.28435643]\n",
            "\n",
            "Updated Weights/Biases:\n",
            "New W1:\n",
            "[[0.10176291 0.20207221]\n",
            " [0.30035258 0.40041444]]\n",
            "New b1: [0.10352582 0.20414441]\n",
            "New W2: [[0.51549398 0.61661183]]\n",
            "New b2: [0.32843564]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Input data\n",
        "x = np.array([0.5, 0.1])\n",
        "y_true = 1  # True label\n",
        "\n",
        "# Initialize weights and biases (as in the example)\n",
        "W1 = np.array([[0.1, 0.2], [0.3, 0.4]])  # Input → Hidden\n",
        "b1 = np.array([0.1, 0.2])                # Hidden layer bias\n",
        "W2 = np.array([[0.5, 0.6]])              # Hidden → Output\n",
        "b2 = np.array([0.3])                     # Output bias\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Forward Pass\n",
        "def forward_pass(x, W1, b1, W2, b2):\n",
        "    # Hidden layer\n",
        "    h_input = np.dot(x, W1) + b1\n",
        "    h_output = sigmoid(h_input)\n",
        "\n",
        "    # Output layer\n",
        "    o_input = np.dot(h_output, W2.T) + b2\n",
        "    o_output = sigmoid(o_input)\n",
        "\n",
        "    return h_input, h_output, o_input, o_output\n",
        "\n",
        "# Backward Pass\n",
        "def backward_pass(x, y_true, h_input, h_output, o_input, o_output, W2):\n",
        "    # Output layer gradients\n",
        "    dL_do_output = - (y_true / o_output) + (1 - y_true) / (1 - o_output)  # ∂L/∂o_output\n",
        "    do_output_do_input = sigmoid_derivative(o_output)                      # ∂o_output/∂o_input\n",
        "    dL_do_input = dL_do_output * do_output_do_input                       # ∂L/∂o_input (δo1)\n",
        "\n",
        "    # Gradients for W2 and b2\n",
        "    dL_dW2 = np.outer(dL_do_input, h_output)  # ∂L/∂W2 = δo1 * h_output\n",
        "    dL_db2 = dL_do_input                      # ∂L/∂b2 = δo1\n",
        "\n",
        "    # Hidden layer gradients\n",
        "    dL_dh_output = np.dot(dL_do_input, W2)    # ∂L/∂h_output = δo1 * W2\n",
        "    dh_output_dh_input = sigmoid_derivative(h_output)  # ∂h_output/∂h_input\n",
        "    dL_dh_input = dL_dh_output * dh_output_dh_input   # ∂L/∂h_input (δh1, δh2)\n",
        "\n",
        "    # Gradients for W1 and b1\n",
        "    dL_dW1 = np.outer(x, dL_dh_input)         # ∂L/∂W1 = x * δh\n",
        "    dL_db1 = dL_dh_input                      # ∂L/∂b1 = δh\n",
        "\n",
        "    return dL_dW1, dL_db1, dL_dW2, dL_db2\n",
        "\n",
        "# Training for 1 iteration\n",
        "h_input, h_output, o_input, o_output = forward_pass(x, W1, b1, W2, b2)\n",
        "dL_dW1, dL_db1, dL_dW2, dL_db2 = backward_pass(x, y_true, h_input, h_output, o_input, o_output, W2)\n",
        "\n",
        "# Update weights and biases\n",
        "W1 -= lr * dL_dW1\n",
        "b1 -= lr * dL_db1\n",
        "W2 -= lr * dL_dW2\n",
        "b2 -= lr * dL_db2\n",
        "\n",
        "# Print results\n",
        "print(f\"Forward Pass Results: {o_output}\")\n",
        "print(f\"Hidden Output: {h_output}\")\n",
        "print(\"\\nGradients:\")\n",
        "print(f\"dL_dW1:\\n{dL_dW1}\")\n",
        "print(f\"dL_db1: {dL_db1}\")\n",
        "print(f\"dL_dW2: {dL_dW2}\")\n",
        "print(f\"dL_db2: {dL_db2}\")\n",
        "print(\"\\nUpdated Weights/Biases:\")\n",
        "print(f\"New W1:\\n{W1}\")\n",
        "print(f\"New b1: {b1}\")\n",
        "print(f\"New W2: {W2}\")\n",
        "print(f\"New b2: {b2}\")"
      ]
    }
  ]
}